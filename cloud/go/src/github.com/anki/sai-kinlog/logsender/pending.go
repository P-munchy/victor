// Copyright 2016 Anki, Inc.

package logsender

import (
	"errors"
	"log"
	"sync"
	"time"

	"github.com/anki/sai-kinlog/logsender/logrecord"
	"github.com/aws/aws-sdk-go/aws"
	"github.com/aws/aws-sdk-go/aws/client"
	"github.com/aws/aws-sdk-go/aws/request"
	"github.com/aws/aws-sdk-go/service/kinesis"
	"github.com/cenkalti/backoff"
)

const (
	throughputExceededCode = "ProvisionedThroughputExceededException"
)

var (
	errPendingIsFull    = errors.New("too much data in pending list")
	errReachedSoftLimit = errors.New("reached buffer soft limit")
)

type pendingSizeLimits struct {
	maxSoftRecordSize int // maximum compressed bytes per record
	maxObjectsPerPut  int // limit for kinesis put
	maxPendingSize    int // max amount of uncompressed data to buffer
}

var defaultPendingSizeLimits = pendingSizeLimits{
	maxSoftRecordSize: 256 * 1024,
	maxObjectsPerPut:  500,
	maxPendingSize:    5 * 1024 * 1024, // 5 MB uncompressed
}

// KinesisError holds a single error response from Kinesis; it's returned
// as a "soft error" during a flush call if an individual record fails
// during a PutRecords response (flush will automatically resend it).
type KinesisError struct {
	Code    string
	Message string
}

// Error returns the error message generated by Kinesis
func (ke KinesisError) Error() string {
	return ke.Message
}

// kinesisRetryer wraps the default aws retryer, adds some logging
// and caps the maximum delay time.
type kinesisRetryer struct {
	client.DefaultRetryer
	errorLog *log.Logger
}

func (kr kinesisRetryer) RetryRules(r *request.Request) time.Duration {
	const maxWait = 10 * time.Second
	d := kr.DefaultRetryer.RetryRules(r)
	if d > maxWait {
		d = maxWait
	}
	if kr.errorLog != nil {
		kr.errorLog.Printf("Kinesis log send retry attempt=%d delay=%s", r.RetryCount, d)
	}
	return d
}

// defaultBackoff generates a reasonable backoff strategy for Kinesis requests
func newDefaultBackoff() backoff.BackOff {
	b := &backoff.ExponentialBackOff{
		InitialInterval:     100 * time.Millisecond,
		RandomizationFactor: backoff.DefaultRandomizationFactor,
		Multiplier:          1.25,
		MaxInterval:         5 * time.Second,
		MaxElapsedTime:      60 * time.Minute,
		Clock:               backoff.SystemClock,
	}
	b.Reset()
	return b
}

// pendingItems holds records that are currently waiting to be sent to Kinesis
// Each unique Metadata key gets its own RecordEncoder to keep data from the
// same source together in-order (unbroken sources also have all their records
// sent to the same partition/shard).
type pendingItems struct {
	recPool    *sync.Pool
	records    map[logrecord.Metadata]*logrecord.Encoder
	rawSize    int
	backoff    backoff.BackOff
	errorLog   *log.Logger
	sizeLimits pendingSizeLimits
}

func newPendingItems(bufPool *sync.Pool, sizeLimits pendingSizeLimits) *pendingItems {
	return &pendingItems{
		recPool:    bufPool,
		records:    make(map[logrecord.Metadata]*logrecord.Encoder),
		backoff:    newDefaultBackoff(),
		sizeLimits: sizeLimits,
	}
}

func (p *pendingItems) allocateBuffer(e logrecord.LogEntry) *logrecord.Encoder {
	buf := p.recPool.Get().(*logrecord.Encoder)
	buf.Reset(e.Metadata)
	return buf
}

func (p *pendingItems) releaseRecords() {
	for _, buf := range p.records {
		p.recPool.Put(buf)
	}
}

func (p *pendingItems) addEntry(e logrecord.LogEntry, softLimit bool) error {
	size := len(e.Data.Data)
	buf := p.records[e.Metadata]
	if buf == nil {
		if len(p.records) >= p.sizeLimits.maxObjectsPerPut || p.rawSize+size > p.sizeLimits.maxPendingSize {
			// must flush
			return errPendingIsFull
		}
		buf = p.allocateBuffer(e)
		p.records[e.Metadata] = buf
	}
	if softLimit && buf.RawSize()+size > p.sizeLimits.maxSoftRecordSize {
		return errReachedSoftLimit
	}
	if err := buf.Encode(e.Data); err != nil {
		return err
	}
	p.rawSize += size
	return nil
}

// flushToKinesis sends the pending records to Kinesis
// it automatically retries partial put failures, returning such events as
// "soft errors"  that require no extra work the part of the caller
//
// hard errors reflect a failure to send some or all data to Kinesis; the
// caller will have to make a decision on how to proceed.
func (p *pendingItems) flushToKinesis(k KinesisPutter, streamName string) (softErrs []KinesisError, hardErr error) {
	recs := make([]*kinesis.PutRecordsRequestEntry, 0, len(p.records))
	for _, buf := range p.records {
		recs = append(recs, &kinesis.PutRecordsRequestEntry{
			Data:         buf.Data(),
			PartitionKey: aws.String(buf.PartitionKey()),
		})
	}

	// use a retryer to handle individual put failures within a large PutRecords request
	retryErr := backoff.Retry(func() (err error) {
		defer func() {
			if p.errorLog != nil {
				if err != nil {
					p.errorLog.Printf("Kinesis log PutRecords failed: %s", err)
				} else if hardErr != nil {
					p.errorLog.Printf("Kinesis log PutRecords hard failure: %s", hardErr)
				}
			}
		}()

		ipt := &kinesis.PutRecordsInput{
			StreamName: aws.String(streamName),
			Records:    recs,
		}
		resp, err := k.PutRecords(ipt)
		if err != nil {
			// Our custom retryer will make many retry attempts; if they all
			// fail then that's a hard error.
			hardErr = err
			return nil
		}
		if aws.Int64Value(resp.FailedRecordCount) == 0 {
			return nil
		}
		repeat := make([]*kinesis.PutRecordsRequestEntry, 0, aws.Int64Value(resp.FailedRecordCount))
		var lasterr error
		for i, result := range resp.Records {
			if result.ErrorCode != nil {
				kerr := KinesisError{
					Code:    aws.StringValue(result.ErrorCode),
					Message: aws.StringValue(result.ErrorMessage),
				}
				lasterr = kerr
				softErrs = append(softErrs, kerr)
				repeat = append(repeat, recs[i])
			}
		}
		recs = repeat
		return lasterr
	}, p.backoff)

	if retryErr != nil {
		hardErr = retryErr
	}

	if hardErr == nil {
		p.releaseRecords()
	}
	return softErrs, hardErr
}

func (p *pendingItems) isEmpty() bool {
	return len(p.records) == 0
}

func newEncoderPool() *sync.Pool {
	return &sync.Pool{
		New: func() interface{} {
			return new(logrecord.Encoder)
		},
	}
}
