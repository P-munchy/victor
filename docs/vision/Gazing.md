# Gazing

Created by Robert Cosgriff Mar 01, 2019

Here lives the work on gazing. Gazing is a catch all term for two elements of pose estimation associated with the human face. Specifically head pose, and eye pose. Both of these are currently computed from the Okao library. The Okao eye gaze documentation is found here and the Okao head direction documentation is found here. This post will cover some the generic algorithm for averaging that is use for both of these as well as the empirical results around accuracy, and where we've tried to expose this in the product and the outcomes associated. For a more granular and specific record the work has been captured under this jira epic.



The first item that was discussed around gazing was eye contact. The idea here was that when the angle between the eyes and image plane was zero, we would consider the associated face to be making eye contact with the robot. Due to the noise associated with the estimate of the angle between the eyes and the image plane however just looking for zero isn't a reliable technique. Fortunately, after looking at the noise associated with this estimate, it was a reasonable assumption that this noise was zero mean. Thus using an average to push down this noise seemed like a reasonable thing to do. In addition a notion of outlier was added to this method of averaging (inspiration via RANSAC). Specifically the average of all the past eye angles relative to the image plane was found, then inliers were found using an predefined threshold. Using these inliers a new average was recomputed, and if this average was sufficiently close to zero we would classify the face as making eye. After tuning these parameters on the robot, this worked reasonably well. Contact found it's way into observing and as well into tracking (ObservingEyeContact, and TrackingEyeContact respectively). The major limitation for eye contact, and more generally the eye angle estimation was found to be distance the face was from the camera. We found that empirically any distance further than about 1.5 meters there was too much noise to be able to reliably use eye angles to determine eye contact with the robot. The expectation was that if we were running this method at a higher resolution, we could expect an increase in this distance, however we haven't tested this. An item to note here is that there is no robot pose associated with this measurement because only care to estimate whether there has been a history of a zero angle between the eyes and the image plane, and to do so we don't need to know if the image plane has moved or not. This is exposed through face world.



The next steps as part of the gazing epic was the depart from simply estimating and angle relative to the image plane and instead have a persistent gaze estimation in the robot world coordinate space. Given the transient nature, and noise statistics we saw during the eye contact work we decided to use the head direction instead of the direction for this persistent gaze estimation. With this technical motivation we then tried to find reasonable product use cases where this sort of work could be show cased or demonstrated. The first of two use cases that we came up with was looking for another persons face, i.e. Vector becomes part of the conversation by following a person's gaze for a certain distance and then searching for a face about that point. The second use case was around when a user is looking at something on the surface Vector is on he would intersect that gaze direction with the ground plane and then demonstrate he was interested in the location of that intersection point. The same meta technique mentioned in the prior paragraph, that was inspired by RANSAC, was used here as well. However, instead of averaging angles we are averaging points in three dimensional space which are either the intersection with a plane, or a sphere. It's not entirely clear that in the case of the intersection with a sphere simple averaging is what we want to do, and instead we may want to instead of limiting the distance intersect the gaze direction with a plane to preserve properties of the noise statistics through an affine transformation, but as it will become clear it's likely that this will all come out in the wash anyway, I simply wanted to mention this.



It's worth discussing these two gaze point estimation separately due their very different product use cases; both use cases are difficult for conflicting reasons. Finding a face using the gaze direction is a harder problem to estimate the exact location a face is directed at because of the angle that a person makes with respect to the robot's camera when interacting with another face. However, this problem is vastly simplified by the closed loop nature of being able to blindly turn and then detect whether or not a face is present in that location. Finding a point of interest on the ground plane suffers from the converse; it's head direction problem is much easier to solve (in comparison, not isolation) but has no way of determining whether or not there is anything interesting at the point it has estimated. This inability to determine whether it was successfully found an interesting point by following the gaze makes it particularly difficult to convey the intent of the robot to the user. Given the open loop nature of the interesting point on the ground plane problem, the majority of the effort went into providing an accurate estimation of the point surface which I'll describe in more specifics next.



There are several limitations around the gaze estimation and what we did to correct for them. Specifically the estimation tends to bias to behind the robot because the further down you look, we either lose your face or instead can provide an accurate estimate because we aren't combining the eye direction with the head direction for a full gaze estimate. To compensate for this we apply a linear shift of the y coordinate to move the gaze estimate to the front of the robot. Additionally another correct measure we took, and this is more of a response to anecdotal experience (which I believe was an issue because we were only using head direction) about the robot overshooting the actual location of the gaze point. To address this overshoot we linearly redistribute the region that typically has the overshoot issues, concretely for example if an angle were to fall in 90-175 degrees we would interpolate that region to lie in 45-135 degrees, those numbers were then tuned via console vars as I got more data points from testing it with other folks. Folks in the feature team seemed to be happy with this however it was unclear how exactly to expose this in a way that would clearly demonstrate to the user what exactly the robot was reacting to. It was agreed that simply putting it in an ambient behavior would not demonstrate clearly enough to the user what was going on, and that he was following your gaze. In an effort to try to expose this more, we tried using a voice command which after testing it out with several folks didn't feel quite right. Unfortunately that is where work stopped for following gaze to a point on the ground plane, until we could find a convincing and meaningful way to expose it in the product. This work is currently on a branch robert/VIC-11275-gazing-v2-prototype, and the an easier to work with for testing behavior is robert/VIC-11275-gazing-v2-prototype-looping which also has all the user testing changes on it as well.



The other use case for following gaze, is to follow the gaze and look for another face. Due to the complexity and thus noise associated with estimating a precise gaze location at a certain distance when a user is gazing at another face, we bucket the turns into four different ones, +/- 90, +/-180. This worked fairly well because of the closed loop nature of this use case. However, there were concerns that because there was no broader context of sound of voice victor has a tendency to just keep looking from face to face, which didn't make him look intelligent. Thus work was postposed until we could leverage this necessary context to make him seem more intelligent. This work is currently in master behind a feature flag that is disabled.